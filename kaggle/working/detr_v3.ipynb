{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with device cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Addresses\n",
    "\n",
    "class Address:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all the addresses used in project\n",
    "        '''\n",
    "        # Inputs\n",
    "        self.data = \"../input/mammography\"\n",
    "        self.processed_data = \"data.pkl\"\n",
    "\n",
    "        # Coco\n",
    "        self.coco = os.path.join(self.data, 'coco_1k')\n",
    "        self.coco_annot = os.path.join(self.coco, 'annotations')\n",
    "        self.coco_annot_train = os.path.join(self.coco_annot, 'instances_train2017.json')\n",
    "        self.coco_annot_val = os.path.join(self.coco_annot, 'instances_val2017.json')\n",
    "        self.coco_img_train = os.path.join(self.coco, 'train2017')\n",
    "        self.coco_img_val = os.path.join(self.coco, 'val2017')\n",
    "\n",
    "        # Test\n",
    "        self.test = os.path.join(self.data, 'test')\n",
    "        self.test_img = os.path.join(self.test, 'images')\n",
    "        self.test_label = os.path.join(self.test, 'labels')\n",
    "        self.predictions = os.path.join(self.test, 'predictions')\n",
    "\n",
    "        # Yolo\n",
    "        self.yolo = os.path.join(self.data, 'yolo_1k')\n",
    "        self.yolo_train = os.path.join(self.yolo, 'train')\n",
    "        self.yolo_train_img = os.path.join(self.yolo_train, 'images')\n",
    "        self.yolo_train_label = os.path.join(self.yolo_train, 'labels')\n",
    "        self.yolo_val = os.path.join(self.yolo, 'val')\n",
    "        self.yolo_val_img = os.path.join(self.yolo_val, 'images')\n",
    "        self.yolo_val_labels = os.path.join(self.yolo_val, 'labels')\n",
    "\n",
    "        # Models\n",
    "        self.result = \"results/\"\n",
    "        self.model_frcnn = os.path.join(self.result, 'frcnn')\n",
    "        self.model_def_detr = os.path.join(self.result, 'deformable_dtr')\n",
    "        self.model_detr = os.path.join(self.result, 'dtr')\n",
    "\n",
    "        # Temp\n",
    "        self.temp = \"temp/\"\n",
    "\n",
    "    def create_dir(self, dir_list = None):\n",
    "        '''\n",
    "        Function to create directories in dir_list. If dir_list is None then create all directories of address.\n",
    "        '''\n",
    "        if dir_list == None:\n",
    "            dir_list = [self.temp, self.result, self.model_frcnn, self.model_def_detr, self.model_detr]\n",
    "        for address in dir_list:\n",
    "            if not os.path.exists(address):\n",
    "                os.mkdir(address)\n",
    "\n",
    "    def _delete_folder_content(self, folder_addr):\n",
    "        '''\n",
    "        Deletes all the content of folder_addr\n",
    "        '''\n",
    "        if os.path.exists(folder_addr):\n",
    "            for file in os.listdir(folder_addr):\n",
    "                address = os.path.join(folder_addr, file)\n",
    "                if os.path.isdir(address):\n",
    "                    self._delete_folder_content(address)\n",
    "                    os.removedirs(address)\n",
    "                else:\n",
    "                    os.remove(address)\n",
    "\n",
    "    def clean(self, file_list = None):\n",
    "        '''\n",
    "        Deletes all the content in file_list\n",
    "        '''\n",
    "        if file_list == None:\n",
    "            file_list = [self.temp]\n",
    "        for address in file_list:\n",
    "            self._delete_folder_content(address)\n",
    "\n",
    "addr = Address()\n",
    "addr.clean()\n",
    "# addr.clean([addr.model_detr])\n",
    "addr.create_dir()\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all Hyperparameters used for training of model\n",
    "        '''\n",
    "        # Training\n",
    "        self.batch_size = 2\n",
    "        self.num_epoch = 50\n",
    "        self.grad_clip = 0.1\n",
    "\n",
    "        # Data\n",
    "        self.num_train = 2240\n",
    "        self.num_val = 218\n",
    "        self.train_step = self.num_epoch*(self.num_train//self.batch_size)\n",
    "        self.resolution = (256, 512)\n",
    "        \n",
    "        # Learning Rate\n",
    "        self.lr = 3e-5\n",
    "        self.backbone_lr = 1e-5\n",
    "        self.weight_decay = 3e-5\n",
    "\n",
    "    def create_report(self, addr):\n",
    "        with open(os.path.join(addr, 'param.txt'), 'w') as file:\n",
    "            file.writelines([\n",
    "                f'Training:',\n",
    "                f'\\n\\tBatch Size:       {self.batch_size}',\n",
    "                f'\\n\\tNum Epoch:        {self.num_epoch}',\n",
    "                f'\\n\\tGrad Clip:        {self.grad_clip}',\n",
    "                f'\\n\\nData:',  \n",
    "                f'\\n\\tNum Train:        {self.num_train}',\n",
    "                f'\\n\\tNum Val:          {self.num_val}',\n",
    "                f'\\n\\tTrain Step:       {self.train_step}',\n",
    "                f'\\n\\tResolution:       {self.resolution}',\n",
    "                f'\\n\\nLearning Rate:',  \n",
    "                f'\\n\\tlr:               {self.lr}',\n",
    "                f'\\n\\tBackbone lr:      {self.backbone_lr}',\n",
    "                f'\\n\\tWeight Decay:     {self.weight_decay}',\n",
    "            ])\n",
    "\n",
    "param = HyperParameters()\n",
    "\n",
    "# Random Seed and CUDA\n",
    "\n",
    "random_seed = 68\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    device = \"cuda\"\n",
    "print(f\"Working with device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import transformers\n",
    "import PIL.ImageEnhance, PIL.ImageFilter\n",
    "import json\n",
    "\n",
    "# !pip install pycocotools\n",
    "\n",
    "model_checkpoint = \"facebook/detr-resnet-50\"\n",
    "\n",
    "class Dataset(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, annotation_file, processor):\n",
    "        super(Dataset, self).__init__(img_folder, annotation_file,)\n",
    "\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(Dataset, self).__getitem__(idx)\n",
    "\n",
    "        img = self.transform_fn(img)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        encoding = self.processor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        encoding[\"pixel_values\"] = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        encoding[\"labels\"] = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def transform_fn(self, img):\n",
    "        '''\n",
    "        Preprocessing\n",
    "        '''\n",
    "        contrast = PIL.ImageEnhance.Contrast(img)\n",
    "        contrast.enhance(3.0)         # Increasing Contrast\n",
    "        image = contrast.image.filter(PIL.ImageFilter.GaussianBlur(radius=5))       # Gaussian Blur\n",
    "        return image\n",
    "\n",
    "class EvalDataSet():\n",
    "    def __init__(self, address_img, address_annot):\n",
    "        '''\n",
    "        Creates Dataset of images on given address.\n",
    "        '''\n",
    "        self.address_img = address_img\n",
    "        self.temp_annotation = None\n",
    "        with open(address_annot, 'rb') as file:\n",
    "            self.annotation = json.load(file)\n",
    "        self.beautify()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if 'image' not in self.annotation[idx]:\n",
    "            img_addr = os.path.join(self.address_img, self.annotation[idx]['file_name'])\n",
    "            img = PIL.Image.open(img_addr)\n",
    "            self.annotation[idx]['image'] = img\n",
    "        return self.annotation[idx]\n",
    "    \n",
    "    def beautify(self):\n",
    "        img_info = self.annotation['images']\n",
    "        annotations = self.annotation['annotations']\n",
    "        id_dict = {}\n",
    "        for img in img_info:\n",
    "            img['objects'] = []\n",
    "            id_dict[img['id']] = img\n",
    "        for annot in annotations:\n",
    "            img = id_dict[annot['image_id']]\n",
    "            img['objects'].append(annot)\n",
    "        self.annotation = id_dict\n",
    "\n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        self.processor = transformers.DetrImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.dataset_train = Dataset(addr.coco_img_train, addr.coco_annot_train, self.processor)\n",
    "        self.dataset_val = Dataset(addr.coco_img_val, addr.coco_annot_val, self.processor)\n",
    "\n",
    "        self.img_train = EvalDataSet(addr.coco_img_train, addr.coco_annot_train)\n",
    "        self.img_val = EvalDataSet(addr.coco_img_val, addr.coco_annot_val)\n",
    "\n",
    "        self.loader_train = torch.utils.data.DataLoader(self.dataset_train, param.batch_size, collate_fn=self.collate_fn, shuffle=True)\n",
    "        self.loader_val = torch.utils.data.DataLoader(self.dataset_val, param.batch_size, collate_fn=self.collate_fn, shuffle=False)\n",
    "\n",
    "        self.id2label = {0: 'mal'}\n",
    "        self.label2id = {'mal': 0}\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        pixel_values = [item['pixel_values'] for item in batch]\n",
    "        encoding = self.processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "        encoding['labels'] = [item['labels'] for item in batch]\n",
    "        return encoding\n",
    "\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForObjectDetection were not initialized from the model checkpoint at facebook/detr-resnet-50 and are newly initialized because the shapes did not match:\n",
      "- class_labels_classifier.weight: found shape torch.Size([92, 256]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- class_labels_classifier.bias: found shape torch.Size([92]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): ResNetBackbone(\n",
       "          (embedder): ResNetEmbeddings(\n",
       "            (embedder): ResNetConvLayer(\n",
       "              (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "              (normalization): DetrFrozenBatchNorm2d()\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (encoder): ResNetEncoder(\n",
       "            (stages): ModuleList(\n",
       "              (0): ResNetStage(\n",
       "                (layers): Sequential(\n",
       "                  (0): ResNetBottleNeckLayer(\n",
       "                    (shortcut): ResNetShortCut(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): DetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ResNetStage(\n",
       "                (layers): Sequential(\n",
       "                  (0): ResNetBottleNeckLayer(\n",
       "                    (shortcut): ResNetShortCut(\n",
       "                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (normalization): DetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (3): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ResNetStage(\n",
       "                (layers): Sequential(\n",
       "                  (0): ResNetBottleNeckLayer(\n",
       "                    (shortcut): ResNetShortCut(\n",
       "                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (normalization): DetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (3): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (4): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (5): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ResNetStage(\n",
       "                (layers): Sequential(\n",
       "                  (0): ResNetBottleNeckLayer(\n",
       "                    (shortcut): ResNetShortCut(\n",
       "                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (normalization): DetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (1): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                  (2): ResNetBottleNeckLayer(\n",
       "                    (shortcut): Identity()\n",
       "                    (layer): Sequential(\n",
       "                      (0): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (1): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): ReLU()\n",
       "                      )\n",
       "                      (2): ResNetConvLayer(\n",
       "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                        (normalization): DetrFrozenBatchNorm2d()\n",
       "                        (activation): Identity()\n",
       "                      )\n",
       "                    )\n",
       "                    (activation): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Learning Model\n",
    "class LearnModel:\n",
    "    def __init__(self, model: torch.nn.Module, model_addr, data=data, param=param, device=device):\n",
    "        '''\n",
    "        Train, Evaluate and Predict\n",
    "        '''\n",
    "        \n",
    "        self.data = data\n",
    "        self.param = param\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.model_addr = model_addr\n",
    "\n",
    "        # Addresses\n",
    "        self.loss_addr = os.path.join(self.model_addr, 'loss.npz')\n",
    "        self.epoch_addr = lambda epoch: os.path.join(self.model_addr, f'model/{epoch}.pth')\n",
    "        addr.create_dir([os.path.join(self.model_addr, 'model')])\n",
    "\n",
    "    def train(self, epoch_log = True, batch_log = True, overwrite = False):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.model.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": param.backbone_lr,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=param.lr, weight_decay=param.weight_decay)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Loss arr\n",
    "        if os.path.exists(self.loss_addr):\n",
    "            loss_arr = np.load(self.loss_addr)\n",
    "            train_loss_arr = list(loss_arr['train'])\n",
    "            val_loss_arr = list(loss_arr['val'])\n",
    "        else:\n",
    "            train_loss_arr = []\n",
    "            val_loss_arr = []\n",
    "\n",
    "        if overwrite:\n",
    "            addr.clean([self.model_addr])\n",
    "        \n",
    "        for epoch in range(self.param.num_epoch):\n",
    "            epoch_addr = self.epoch_addr(epoch)\n",
    "\n",
    "            # Loading Model if present\n",
    "            if os.path.exists(epoch_addr):\n",
    "                self.model.load_state_dict(torch.load(epoch_addr), strict=False)\n",
    "                print(f\"Loaded model and scheduler at epoch {epoch}\")\n",
    "                continue\n",
    "\n",
    "            # Training Model\n",
    "            train_loss = self.train_epoch(optimizer, batch_log=batch_log)\n",
    "            if epoch_log:\n",
    "                print(f'Epoch: {epoch}\\tTrain Loss: {train_loss}\\tTime: {time.time()-start_time}')\n",
    "\n",
    "            # Validating Model\n",
    "            val_loss = self.validate_epoch(self.data.loader_val, batch_log=False)\n",
    "            if epoch_log:\n",
    "                print(f'Epoch: {epoch}\\tVal Loss: {val_loss}\\tTime: {time.time()-start_time}')\n",
    "\n",
    "            # Saving data\n",
    "            train_loss_arr.append(train_loss)\n",
    "            val_loss_arr.append(val_loss)\n",
    "            np.savez_compressed(self.loss_addr, train=np.array(train_loss_arr), val=np.array(val_loss_arr))     # Saving Loss Array\n",
    "            torch.save(self.model.state_dict(), epoch_addr)     # Saving Model\n",
    "\n",
    "            # Printing blank line between each epoch in Log\n",
    "            if epoch_log:\n",
    "                print()\n",
    "        \n",
    "    def train_epoch(self, optimizer, batch_log):\n",
    "        '''\n",
    "        Trains model for one epoch\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        batch_ct = 0\n",
    "        dataloader = self.data.loader_train\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.train()          # Set Model to train Mode\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # Copying data to cuda\n",
    "            pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "            pixel_mask = batch[\"pixel_mask\"].to(self.device)\n",
    "            labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "            # Forward Propagation\n",
    "            outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "            # Computing Loss\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Back Propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.param.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update batch count\n",
    "            batch_ct += 1\n",
    "\n",
    "            if batch_log and batch_ct%25 == 0:\n",
    "                print(f\"\\tBatch {batch_ct}\\tLoss: {epoch_loss/batch_ct}\\tTime: {time.time()-start_time}\")\n",
    "\n",
    "        return epoch_loss/batch_ct\n",
    "    \n",
    "    def validate_epoch(self, dataloader, batch_log):\n",
    "        '''\n",
    "        Calculates Loss on data in given dataloader\n",
    "        '''\n",
    "        epoch_loss = 0\n",
    "        batch_ct = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.eval()           # Set Model to eval mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Copying data to cuda\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                pixel_mask = batch[\"pixel_mask\"].to(self.device)\n",
    "                labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "                # Forward Propagation\n",
    "                outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "                # Computing Loss\n",
    "                loss = outputs.loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Update batch count\n",
    "                batch_ct += 1\n",
    "\n",
    "                if batch_log and batch_ct%25 == 0:\n",
    "                    print(f\"\\tBatch {batch_ct}\\tLoss: {epoch_loss/batch_ct}\\tTime: {time.time()-start_time}\")\n",
    "\n",
    "        return epoch_loss/batch_ct\n",
    "\n",
    "    def infer(self):\n",
    "        iou_threshold = [0, 0.1, 0.25, 0.5, 1]\n",
    "        nms_addr = os.path.join(addr.model_def_detr, 'nms')\n",
    "        addr.create_dir([nms_addr])\n",
    "\n",
    "        def plot_results(pil_img, boxes, orig_boxes, file_addr):\n",
    "            plt.figure(figsize=(5, 10))\n",
    "            plt.imshow(pil_img)\n",
    "            ax = plt.gca()\n",
    "            for (xmin, ymin, xmax, ymax) in boxes.tolist():\n",
    "                ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                        fill=False, color='r', linewidth=1))\n",
    "            for (xmin, ymin, xwidth, ywidth) in orig_boxes:\n",
    "                ax.add_patch(plt.Rectangle((xmin, ymin), xwidth, ywidth,\n",
    "                                        fill=False, color='b', linewidth=1))\n",
    "            plt.axis('off')\n",
    "            plt.savefig(file_addr)\n",
    "            plt.close()\n",
    "\n",
    "        def convert_to_xywh(boxes):\n",
    "            xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "            return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
    "\n",
    "        def prepare_for_coco_detection(predictions):\n",
    "            coco_results = []\n",
    "            for original_id, prediction in predictions.items():\n",
    "                if len(prediction) == 0:\n",
    "                    continue\n",
    "\n",
    "                boxes = prediction[\"boxes\"]\n",
    "                boxes = convert_to_xywh(boxes).tolist()\n",
    "                scores = prediction[\"scores\"].tolist()\n",
    "                labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "                coco_result_single = [{\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": labels[k],\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": scores[k],\n",
    "                } for k, box in enumerate(boxes)]\n",
    "\n",
    "                coco_results.append(coco_result_single)\n",
    "\n",
    "            return coco_results\n",
    "\n",
    "        self.model.eval()\n",
    "        nms_addr = os.path.join(self.model_addr, 'nms')\n",
    "        pred_addr = os.path.join(self.model_addr, 'pred')\n",
    "        addr.create_dir([nms_addr, pred_addr])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data.loader_val:\n",
    "                # Copying data to cuda\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                pixel_mask = batch[\"pixel_mask\"].to(self.device)\n",
    "                labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "                # Forward Propagation\n",
    "                outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "                target_sizes = torch.stack([target['orig_size'] for target in labels], dim = 0)\n",
    "\n",
    "                # Storing Predictions\n",
    "                results = data.processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.0)\n",
    "                predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "                predictions = prepare_for_coco_detection(predictions)\n",
    "\n",
    "                # Plotting with nms\n",
    "                for ind in range(len(results)):\n",
    "                    single_result = results[ind]\n",
    "                    img_id = labels[ind]['image_id'].cpu().item()\n",
    "                    img_data = data.img_val[img_id]\n",
    "                    filename = img_data['file_name']\n",
    "                    height, width = img_data['height'], img_data['width']\n",
    "                    img = img_data['image']\n",
    "                    orig_boxes = [elem['bbox'] for elem in img_data['objects']]\n",
    "\n",
    "                    for i in range(len(iou_threshold)):\n",
    "                        ind = torchvision.ops.nms(single_result['boxes'].cpu(), single_result['scores'].cpu(), iou_threshold[i])\n",
    "                        address = os.path.join(nms_addr, filename[:-4])\n",
    "                        addr.create_dir([address])\n",
    "                        plot_results(img, single_result['boxes'][ind], orig_boxes, os.path.join(address, f\"{iou_threshold[i]}.png\"))\n",
    "                        if iou_threshold[i] == 0:\n",
    "                            with open(os.path.join(pred_addr, f'{filename[:-4]}.txt'), 'w') as file:\n",
    "                                for box, score in zip(single_result['boxes'][ind].cpu().tolist(), single_result['scores'].cpu().tolist()):\n",
    "                                    x = (box[0] + box[2])/(2*width)\n",
    "                                    y = (box[1] + box[3])/(2*height)\n",
    "                                    w = (box[2] - box[0])/(width)\n",
    "                                    h = (box[3] - box[1])/(height)\n",
    "                                    file.write(f'0 {x} {y} {w} {h} {score}\\n')\n",
    "\n",
    "    def generate_heatmap(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in data.loader_val:\n",
    "                # Copying data to cuda\n",
    "                pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "                pixel_mask = batch[\"pixel_mask\"].to(self.device)\n",
    "                labels = [{k: v for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "                target_sizes = torch.stack([target['orig_size'] for target in labels], dim = 0)\n",
    "\n",
    "                conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "\n",
    "                hooks = [\n",
    "                    self.model.model.backbone.conv_encoder.model.encoder.stages[-1].register_forward_hook(\n",
    "                        lambda self, input, output: conv_features.append(output)\n",
    "                    ),\n",
    "                    self.model.model.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "                        lambda self, input, output: enc_attn_weights.append(output[1])\n",
    "                    ),\n",
    "                    self.model.model.decoder.layers[-1].self_attn.register_forward_hook(\n",
    "                        lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "                    ),\n",
    "                ]\n",
    "\n",
    "                # Forward Propagation\n",
    "                outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "                results = data.processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0)\n",
    "                bboxes_scaled = [[result['boxes'][torch.argmax(result['scores'])]] for result in results]\n",
    "\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "                \n",
    "                # don't need the list anymore\n",
    "                conv_features = conv_features[0]\n",
    "                enc_attn_weights = enc_attn_weights[0]\n",
    "                dec_attn_weights = dec_attn_weights[0]\n",
    "\n",
    "                # print(enc_attn_weights.shape)\n",
    "                print(dec_attn_weights.shape)\n",
    "                break\n",
    "                \n",
    "                for ind in range(conv_features.shape[0]):\n",
    "                    h, w = conv_features[ind].shape[-2:]\n",
    "\n",
    "                    fig, axs = plt.subplots(ncols=len(bboxes_scaled[ind]), nrows=2, figsize=(22, 7))\n",
    "                    for ax_i, (xmin, ymin, xmax, ymax) in zip(axs.T, bboxes_scaled[ind]):\n",
    "                        ax = ax_i[0]\n",
    "                        ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
    "                        ax.axis('off')\n",
    "                        ax.set_title(f'query id: {idx.item()}')\n",
    "                        ax = ax_i[1]\n",
    "                        ax.imshow(im)\n",
    "                        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                                fill=False, color='blue', linewidth=3))\n",
    "                        ax.axis('off')\n",
    "                        ax.set_title(CLASSES[probas[idx].argmax()])\n",
    "                    fig.tight_layout()\n",
    "\n",
    "                break\n",
    "\n",
    "    def plot_loss(self, addr = None):\n",
    "        '''\n",
    "        Plots Loss vs number of epochs\n",
    "        '''\n",
    "        if not os.path.exists(self.loss_addr):\n",
    "            raise Exception(\"No Loss Array\")\n",
    "        loss_arr = np.load(self.loss_addr)\n",
    "        train_arr, val_arr = loss_arr['train'], loss_arr['val']\n",
    "        num_epoch = train_arr.shape[0]\n",
    "        x_arr = np.linspace(1, num_epoch, num_epoch)\n",
    "\n",
    "        if addr is None:\n",
    "            addr = os.path.join(self.model_addr, 'loss_curve')\n",
    "\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.plot(x_arr, train_arr, label='Train')\n",
    "        plt.plot(x_arr, val_arr, label='Val')\n",
    "        plt.legend()\n",
    "        plt.savefig(addr)\n",
    "\n",
    "    def best_model(self):\n",
    "        '''\n",
    "        Returns Best Model as well as changes self.model in place to best model\n",
    "        '''\n",
    "        if not os.path.exists(self.loss_addr):\n",
    "            raise Exception(\"No Loss Array\")\n",
    "        loss_arr = np.load(self.loss_addr)\n",
    "        best_epoch = np.argmin(loss_arr['val'])\n",
    "        self.model.load_state_dict(torch.load(self.epoch_addr(best_epoch)), strict=False)\n",
    "        return self.model\n",
    "\n",
    "    def create_report(self):\n",
    "        self.param.create_report(self.model_addr)\n",
    "\n",
    "model = transformers.DetrForObjectDetection.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    revision = \"no_timm\",\n",
    "    num_labels = len(data.id2label),\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)\n",
    "\n",
    "learner = LearnModel(model, addr.model_detr)\n",
    "# learner.create_report() \n",
    "# learner.train()\n",
    "# learner.plot_loss()\n",
    "# learner.best_model()\n",
    "learner.infer()\n",
    "# learner.generate_heatmap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
