{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch.cuda, torch.utils.data, torch.nn, torch.optim, torch\n",
    "import torchvision.transforms, torchvision.datasets.folder, torchvision.utils, torchvision.models\n",
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addresses\n",
    "\n",
    "class Address:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all the addresses used in project\n",
    "        '''\n",
    "        # Inputs\n",
    "        self.data = \"../input/mammography\"\n",
    "        self.processed_data = \"data.pkl\"\n",
    "\n",
    "        # Coco\n",
    "        self.coco = os.path.join(self.data, 'coco_1k')\n",
    "        self.coco_annot = os.path.join(self.coco, 'annotations')\n",
    "        self.coco_annot_train = os.path.join(self.coco_annot, 'instances_train2017.json')\n",
    "        self.coco_annot_val = os.path.join(self.coco_annot, 'instances_val2017.json')\n",
    "        self.coco_img_train = os.path.join(self.coco, 'train2017')\n",
    "        self.coco_img_val = os.path.join(self.coco, 'val2017')\n",
    "\n",
    "        # Test\n",
    "        self.test = os.path.join(self.data, 'test')\n",
    "        self.test_img = os.path.join(self.test, 'images')\n",
    "        self.test_label = os.path.join(self.test, 'labels')\n",
    "        self.predictions = os.path.join(self.test, 'predictions')\n",
    "\n",
    "        # Yolo\n",
    "        self.yolo = os.path.join(self.data, 'yolo_1k')\n",
    "        self.yolo_train = os.path.join(self.yolo, 'train')\n",
    "        self.yolo_train_img = os.path.join(self.yolo_train, 'images')\n",
    "        self.yolo_train_label = os.path.join(self.yolo_train, 'labels')\n",
    "        self.yolo_val = os.path.join(self.yolo, 'val')\n",
    "        self.yolo_val_img = os.path.join(self.yolo_val, 'images')\n",
    "        self.yolo_val_labels = os.path.join(self.yolo_val, 'labels')\n",
    "\n",
    "        # Models\n",
    "        self.result = \"results/\"\n",
    "        self.model_frcnn = os.path.join(self.result, 'frcnn')\n",
    "        self.model_def_detr = os.path.join(self.result, 'deformable_dtr')\n",
    "\n",
    "        # Temp\n",
    "        self.temp = \"temp/\"\n",
    "\n",
    "    def create_dir(self, dir_list = None):\n",
    "        '''\n",
    "        Function to create directories in dir_list. If dir_list is None then create all directories of address.\n",
    "        '''\n",
    "        if dir_list == None:\n",
    "            dir_list = [self.temp, self.result, self.model_frcnn, self.model_def_detr]\n",
    "        for address in dir_list:\n",
    "            if not os.path.exists(address):\n",
    "                os.mkdir(address)\n",
    "\n",
    "    def _delete_folder_content(self, folder_addr):\n",
    "        '''\n",
    "        Deletes all the content of folder_addr\n",
    "        '''\n",
    "        if os.path.exists(folder_addr):\n",
    "            for file in os.listdir(folder_addr):\n",
    "                address = os.path.join(folder_addr, file)\n",
    "                if os.path.isdir(address):\n",
    "                    self._delete_folder_content(address)\n",
    "                    os.removedirs(address)\n",
    "                else:\n",
    "                    os.remove(address)\n",
    "\n",
    "    def clean(self, file_list = None):\n",
    "        '''\n",
    "        Deletes all the content in file_list\n",
    "        '''\n",
    "        if file_list == None:\n",
    "            file_list = [self.temp]\n",
    "        for address in file_list:\n",
    "            self._delete_folder_content(address)\n",
    "\n",
    "addr = Address()\n",
    "addr.clean()\n",
    "# addr.clean([addr.result])\n",
    "addr.create_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all Hyperparameters used for training of model\n",
    "        '''\n",
    "        # Training\n",
    "        self.batch_size = 2\n",
    "        self.num_epoch = 10\n",
    "        self.grad_clip = 1.0\n",
    "\n",
    "        # Data\n",
    "        self.num_train = 40000\n",
    "        self.num_val = 10000\n",
    "        self.train_step = self.num_epoch*(self.num_train//self.batch_size)\n",
    "        self.resolution = (512, 1024)\n",
    "        \n",
    "        # Learning Rate\n",
    "        self.lr = 1e-4\n",
    "        self.warmup_step = self.train_step//40\n",
    "        self.decay_step = self.train_step//2\n",
    "        self.decay_rate = 0.5\n",
    "\n",
    "    def lr_schedule(self, step):\n",
    "        '''\n",
    "        Getting learning rate as function of train steps completed (Exponential decay with linear warmup)\n",
    "        '''\n",
    "        if step <= self.warmup_step:\n",
    "            return step/self.warmup_step\n",
    "        else:\n",
    "            return self.decay_rate**((step-self.warmup_step)/self.decay_step)\n",
    "\n",
    "    def create_report(self, addr):\n",
    "        with open(os.path.join(addr, 'param.txt'), 'w') as file:\n",
    "            file.writelines([\n",
    "                f'Training:',\n",
    "                f'\\n\\tBatch Size:       {self.batch_size}',\n",
    "                f'\\n\\tNum Epoch:        {self.num_epoch}',\n",
    "                f'\\n\\tGrad Clip:        {self.grad_clip}',\n",
    "                f'\\n\\nData:',  \n",
    "                f'\\n\\tNum Train:        {self.num_train}',\n",
    "                f'\\n\\tNum Val:          {self.num_val}',\n",
    "                f'\\n\\tTrain Step:       {self.train_step}',\n",
    "                f'\\n\\tResolution:       {self.resolution}',\n",
    "                f'\\n\\nLearning Rate:',  \n",
    "                f'\\n\\tlr:               {self.lr}',\n",
    "                f'\\n\\tWarmup Step:      {self.warmup_step}',\n",
    "                f'\\n\\tDecay Step:       {self.decay_step}',\n",
    "                f'\\n\\tDecay Rate:       {self.decay_rate}',\n",
    "            ])\n",
    "\n",
    "param = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with device cpu\n"
     ]
    }
   ],
   "source": [
    "# Random Seed and CUDA\n",
    "\n",
    "random_seed = 68\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(random_seed)\n",
    "#     device = \"cuda\"\n",
    "print(f\"Working with device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, address: Address, param: HyperParameters):\n",
    "        '''\n",
    "        Creates DataLoader and DataSet for both train and val split\n",
    "        '''\n",
    "        self.address = address\n",
    "        self.param = param\n",
    "\n",
    "        # Transform\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize(param.resolution),\n",
    "                                                         torchvision.transforms.ToTensor()])\n",
    "\n",
    "        # Dataset\n",
    "        self.dataset_train = torchvision.datasets.CocoDetection(self.address.coco_img_train,\n",
    "                                                                self.address.coco_annot_train,\n",
    "                                                                transform=self.transform,\n",
    "                                                                target_transform=self.trg_transform)\n",
    "        self.dataset_val = torchvision.datasets.CocoDetection(self.address.coco_img_val,\n",
    "                                                              self.address.coco_annot_val,\n",
    "                                                              transform=self.transform,\n",
    "                                                              target_transform=self.trg_transform)\n",
    "\n",
    "        # DataLoader\n",
    "        self.loader_train = torch.utils.data.DataLoader(self.dataset_train, batch_size=param.batch_size, shuffle=True)\n",
    "        self.loader_val = torch.utils.data.DataLoader(self.dataset_val, batch_size=param.batch_size, shuffle=False)\n",
    "    \n",
    "    def trg_transform(self, elem):\n",
    "        if 'bbox' in elem:\n",
    "            return_dict = {'bbox': torch.tensor(elem['bbox'])}\n",
    "            return_dict['bbox'][2] += return_dict['bbox'][0]\n",
    "            return_dict['bbox'][3] += return_dict['bbox'][1]\n",
    "            return return_dict\n",
    "        return {'bbox': torch.tensor([]).reshape(0)}\n",
    "  \n",
    "    def collate(self, batch):\n",
    "        img_list = []\n",
    "        trg_list = {'bbox': []}\n",
    "        for elem in batch:\n",
    "            img_list.append(elem[0])\n",
    "            trg_list['bbox'].append(elem[1]['bbox'])\n",
    "        trg_list['bbox'] = torch.stack(trg_list['bbox'])\n",
    "        return torch.stack(img_list), trg_list\n",
    "\n",
    "data = Data(addr, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ris04\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/11200 [01:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DeformableDetrModel object argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 37\u001b[0m\n\u001b[0;32m     28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     30\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mcollate\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ris04\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ris04\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2124\u001b[0m ):\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ris04\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ris04\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: DeformableDetrModel object argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "# Define Deformable DETR model configuration\n",
    "config = transformers.DeformableDetrConfig.from_pretrained(\"SenseTime/deformable-detr\", num_labels=2)\n",
    "\n",
    "# Initialize Deformable DETR model\n",
    "model = transformers.DeformableDetrModel(config)\n",
    "\n",
    "# Training arguments\n",
    "model_addr = os.path.join(addr.model_def_detr, 'model')\n",
    "log_addr = os.path.join(addr.model_def_detr, 'logs')\n",
    "addr.create_dir([model_addr, log_addr])\n",
    "training_args = transformers.TrainingArguments(output_dir = model_addr,\n",
    "                                               num_train_epochs = param.num_epoch,\n",
    "                                               per_device_train_batch_size = param.batch_size,\n",
    "                                               learning_rate = param.lr,\n",
    "                                               warmup_steps = param.warmup_step,\n",
    "                                               evaluation_strategy ='epoch',\n",
    "                                               save_strategy = 'epoch',\n",
    "                                               logging_dir = log_addr,\n",
    "                                               logging_strategy = 'steps',\n",
    "                                               logging_steps = 1,\n",
    "                                               use_cpu = False,\n",
    "                                               seed = random_seed,\n",
    "                                               load_best_model_at_end =True)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data.dataset_train,\n",
    "    eval_dataset=data.dataset_val,\n",
    "    data_collator=data.collate\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
