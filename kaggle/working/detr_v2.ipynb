{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wand_key = '1c0992a8cc7785f33c39e7f1b62841cf07d7f19e'\n",
    "wandb.login(key = wand_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Addresses\n",
    "\n",
    "class Address:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all the addresses used in project\n",
    "        '''\n",
    "        # Inputs\n",
    "        self.data = \"../input/mammography\"\n",
    "        self.processed_data = \"data.pkl\"\n",
    "\n",
    "        # Coco\n",
    "        self.coco = os.path.join(self.data, 'coco_1k')\n",
    "        self.coco_annot = os.path.join(self.coco, 'annotations')\n",
    "        self.coco_annot_train = os.path.join(self.coco_annot, 'instances_train2017.json')\n",
    "        self.coco_annot_val = os.path.join(self.coco_annot, 'instances_val2017.json')\n",
    "        self.coco_img_train = os.path.join(self.coco, 'train2017')\n",
    "        self.coco_img_val = os.path.join(self.coco, 'val2017')\n",
    "\n",
    "        # Test\n",
    "        self.test = os.path.join(self.data, 'test')\n",
    "        self.test_img = os.path.join(self.test, 'images')\n",
    "        self.test_label = os.path.join(self.test, 'labels')\n",
    "        self.predictions = os.path.join(self.test, 'predictions')\n",
    "\n",
    "        # Yolo\n",
    "        self.yolo = os.path.join(self.data, 'yolo_1k')\n",
    "        self.yolo_train = os.path.join(self.yolo, 'train')\n",
    "        self.yolo_train_img = os.path.join(self.yolo_train, 'images')\n",
    "        self.yolo_train_label = os.path.join(self.yolo_train, 'labels')\n",
    "        self.yolo_val = os.path.join(self.yolo, 'val')\n",
    "        self.yolo_val_img = os.path.join(self.yolo_val, 'images')\n",
    "        self.yolo_val_labels = os.path.join(self.yolo_val, 'labels')\n",
    "\n",
    "        # Models\n",
    "        self.result = \"results/\"\n",
    "        self.model_frcnn = os.path.join(self.result, 'frcnn')\n",
    "        self.model_def_detr = os.path.join(self.result, 'deformable_dtr')\n",
    "\n",
    "        # Temp\n",
    "        self.temp = \"temp/\"\n",
    "\n",
    "    def create_dir(self, dir_list = None):\n",
    "        '''\n",
    "        Function to create directories in dir_list. If dir_list is None then create all directories of address.\n",
    "        '''\n",
    "        if dir_list == None:\n",
    "            dir_list = [self.temp, self.result, self.model_frcnn, self.model_def_detr]\n",
    "        for address in dir_list:\n",
    "            if not os.path.exists(address):\n",
    "                os.mkdir(address)\n",
    "\n",
    "    def _delete_folder_content(self, folder_addr):\n",
    "        '''\n",
    "        Deletes all the content of folder_addr\n",
    "        '''\n",
    "        if os.path.exists(folder_addr):\n",
    "            for file in os.listdir(folder_addr):\n",
    "                address = os.path.join(folder_addr, file)\n",
    "                if os.path.isdir(address):\n",
    "                    self._delete_folder_content(address)\n",
    "                    os.removedirs(address)\n",
    "                else:\n",
    "                    os.remove(address)\n",
    "\n",
    "    def clean(self, file_list = None):\n",
    "        '''\n",
    "        Deletes all the content in file_list\n",
    "        '''\n",
    "        if file_list == None:\n",
    "            file_list = [self.temp]\n",
    "        for address in file_list:\n",
    "            self._delete_folder_content(address)\n",
    "\n",
    "addr = Address()\n",
    "addr.clean()\n",
    "# addr.clean([addr.model_def_detr])\n",
    "addr.create_dir()\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Stores all Hyperparameters used for training of model\n",
    "        '''\n",
    "        # Training\n",
    "        self.batch_size = 16\n",
    "        self.num_epoch = 100\n",
    "        self.grad_clip = 1.0\n",
    "\n",
    "        # Data\n",
    "        self.num_train = 2240\n",
    "        self.num_val = 218\n",
    "        self.train_step = self.num_epoch*(self.num_train//self.batch_size)\n",
    "        self.resolution = (256, 512)\n",
    "        \n",
    "        # Learning Rate\n",
    "        self.lr = 1e-5\n",
    "        self.warmup_step = self.train_step//40\n",
    "        self.decay_step = self.train_step//2\n",
    "        self.decay_rate = 0.5\n",
    "\n",
    "    def lr_schedule(self, step):\n",
    "        '''\n",
    "        Getting learning rate as function of train steps completed (Exponential decay with linear warmup)\n",
    "        '''\n",
    "        if step <= self.warmup_step:\n",
    "            return step/self.warmup_step\n",
    "        else:\n",
    "            return self.decay_rate**((step-self.warmup_step)/self.decay_step)\n",
    "\n",
    "    def create_report(self, addr):\n",
    "        with open(os.path.join(addr, 'param.txt'), 'w') as file:\n",
    "            file.writelines([\n",
    "                f'Training:',\n",
    "                f'\\n\\tBatch Size:       {self.batch_size}',\n",
    "                f'\\n\\tNum Epoch:        {self.num_epoch}',\n",
    "                f'\\n\\tGrad Clip:        {self.grad_clip}',\n",
    "                f'\\n\\nData:',  \n",
    "                f'\\n\\tNum Train:        {self.num_train}',\n",
    "                f'\\n\\tNum Val:          {self.num_val}',\n",
    "                f'\\n\\tTrain Step:       {self.train_step}',\n",
    "                f'\\n\\tResolution:       {self.resolution}',\n",
    "                f'\\n\\nLearning Rate:',  \n",
    "                f'\\n\\tlr:               {self.lr}',\n",
    "                f'\\n\\tWarmup Step:      {self.warmup_step}',\n",
    "                f'\\n\\tDecay Step:       {self.decay_step}',\n",
    "                f'\\n\\tDecay Rate:       {self.decay_rate}',\n",
    "            ])\n",
    "\n",
    "param = HyperParameters()\n",
    "\n",
    "# Random Seed and CUDA\n",
    "\n",
    "random_seed = 68\n",
    "device = \"cpu\"\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    device = \"cuda\"\n",
    "print(f\"Working with device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchvision\n",
    "import transformers\n",
    "\n",
    "# Dataset\n",
    "\n",
    "# model_checkpoint = \"SenseTime/deformable-detr\"\n",
    "model_checkpoint = \"SenseTime/deformable-detr-single-scale\"\n",
    "# model_checkpoint = \"facebook/deformable-detr-box-supervised\"\n",
    "\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, address_img, address_annot):\n",
    "        '''\n",
    "        Creates Dataset of images on given address.\n",
    "        '''\n",
    "        self.address_img = address_img\n",
    "        self.temp_annotation = None\n",
    "        with open(address_annot, 'rb') as file:\n",
    "            self.annotation = json.load(file)\n",
    "        self.label2id = {cat['name']: cat['id'] for cat in self.annotation['categories']}\n",
    "        self.id2label = {cat['id']: cat['name'] for cat in self.annotation['categories']}\n",
    "        self.preprocess = torchvision.transforms.Compose([torchvision.transforms.GaussianBlur(5),\n",
    "                                                          torchvision.transforms.ColorJitter(contrast=50),\n",
    "                                                          torchvision.transforms.Grayscale()])\n",
    "        self.beautify()\n",
    "        self.feature_extractor = transformers.DeformableDetrFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "        self.feature_extractor.size = {'shortest_edge': param.resolution[0], 'longest_edge': param.resolution[1]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if 'image' not in self.annotation[idx]:\n",
    "            img_addr = os.path.join(self.address_img, self.annotation[idx]['file_name'])\n",
    "            img = torchvision.datasets.folder.default_loader(img_addr)\n",
    "            self.annotation[idx]['image'] = img\n",
    "        return self.transform(self.annotation[idx])\n",
    "    \n",
    "    def beautify(self):\n",
    "        img_info = self.annotation['images']\n",
    "        annotations = self.annotation['annotations']\n",
    "        id_dict = {}\n",
    "        for img in img_info:\n",
    "            img['objects'] = []\n",
    "            id_dict[img['id']] = img\n",
    "        for annot in annotations:\n",
    "            img = id_dict[annot['image_id']]\n",
    "            img['objects'].append(annot)\n",
    "        self.annotation = sorted(id_dict.values(), key=lambda item: item['id'])\n",
    "        self.file_dict = {elem['file_name']: elem for elem in self.annotation}\n",
    "\n",
    "    def transform(self, elem):\n",
    "        # images = self.preprocess(elem[\"image\"]).unsqueeze(0)\n",
    "        images = elem[\"image\"]\n",
    "        ids_ = elem[\"id\"]\n",
    "        objects = elem[\"objects\"]\n",
    "        targets = {\"image_id\": ids_, \"annotations\": objects}\n",
    "        feature = self.feature_extractor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "        feature['file_name'] = elem['file_name']\n",
    "        return feature\n",
    "\n",
    "    def collate(self, batch):\n",
    "        pixel_values = [item[\"pixel_values\"].squeeze(0) for item in batch]\n",
    "        encoding = self.feature_extractor.pad(\n",
    "            pixel_values, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = [item[\"labels\"][0] for item in batch]\n",
    "        batch = {} # collated batch  \n",
    "        batch['pixel_values'] = encoding['pixel_values']\n",
    "        batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "dataset_train = DataSet(addr.coco_img_train, addr.coco_annot_train)\n",
    "dataset_val = DataSet(addr.coco_img_val, addr.coco_annot_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "config = transformers.DeformableDetrConfig.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=1, \n",
    "    id2label=dataset_train.id2label,\n",
    "    label2id=dataset_train.label2id,\n",
    ")\n",
    "\n",
    "model = transformers.DeformableDetrForObjectDetection(config)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir = addr.model_def_detr,\n",
    "    num_train_epochs = param.num_epoch,\n",
    "    per_device_train_batch_size = param.batch_size,\n",
    "    learning_rate = param.lr,\n",
    "    warmup_steps = param.warmup_step,\n",
    "    max_grad_norm=param.grad_clip,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    save_total_limit = 2,\n",
    "    logging_strategy = 'epoch',\n",
    "    use_cpu = False,\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,\n",
    "    tokenizer=dataset_train.feature_extractor,\n",
    "    data_collator=dataset_train.collate,\n",
    ")\n",
    "\n",
    "print(\"Started Training\")\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "iou_threshold = [0, 0.1, 0.25, 0.5, 1]\n",
    "nms_addr = os.path.join(addr.model_def_detr, 'nms')\n",
    "addr.create_dir([nms_addr])\n",
    "\n",
    "def plot_results(pil_img, boxes, orig_boxes, file_addr):\n",
    "    plt.figure(figsize=(5, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    for (xmin, ymin, xmax, ymax) in boxes.tolist():\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color='r', linewidth=1))\n",
    "    for (xcen, ycen, xwidth, ywidth) in orig_boxes:\n",
    "        ax.add_patch(plt.Rectangle((xcen-xwidth/2, ycen-ywidth/2), xwidth, ywidth,\n",
    "                                   fill=False, color='b', linewidth=1))\n",
    "    plt.axis('off')\n",
    "    plt.savefig(file_addr)\n",
    "    plt.close()\n",
    "\n",
    "final_model = os.path.join(addr.model_def_detr, f'checkpoint')\n",
    "model = transformers.DeformableDetrForObjectDetection.from_pretrained(final_model).to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in dataset_train:\n",
    "        # Loading data\n",
    "        filename = data['file_name']\n",
    "        del data['file_name']\n",
    "\n",
    "        # Getting original box\n",
    "        orig_data = dataset_train.file_dict[filename]\n",
    "        orig_boxes = [elem['bbox'] for elem in orig_data['objects']]\n",
    "\n",
    "        # Passing through the model\n",
    "        features = dataset_train.feature_extractor(images=orig_data['image'])\n",
    "        features['pixel_values'] = torch.tensor(np.stack(features['pixel_values'])).to(device)\n",
    "        features['pixel_mask'] = torch.tensor(np.stack(features['pixel_mask'])).to(device)\n",
    "        output = model(**features)\n",
    "        img = PIL.Image.open(os.path.join(addr.coco_img_train, filename))\n",
    "        target_sizes = torch.tensor([img.size[::-1]])\n",
    "\n",
    "        # Plotting with nms\n",
    "        results = dataset_train.feature_extractor.post_process_object_detection(output, target_sizes=target_sizes, threshold=0.0)[0]\n",
    "        results['boxes'] = results['boxes'].cpu()\n",
    "        results['scores'] = results['scores'].cpu()\n",
    "        for i in range(len(iou_threshold)):\n",
    "            ind = torchvision.ops.nms(results['boxes'], results['scores'], iou_threshold[i])\n",
    "            address = os.path.join(nms_addr, filename[:-4])\n",
    "            addr.create_dir([address])\n",
    "            plot_results(img, results['boxes'][ind], orig_boxes, os.path.join(address, f\"{iou_threshold[i]}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
